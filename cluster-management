Shard Allocation Awareness
---------------------------------------
node.attr.my_rack_id=rack1
node.attr.my_rack_id=rack2

PUT _cluster/settings
{
  "persistent": {
     "cluster.routing.allocation.awareness.attributes": "my_rack_id"
  }
}

Configure Forced Awareness
----------------------------------
PUT _cluster/settings
{
  "persistent": {
    "cluster": {
        "routing": {
           "allocation.awareness.attributes": "my_rack_id",
           "allocation.awareness.force.my_rack_id.values": "rack1,rack2"
        }
      }
  }
}

Installing Plugins
-------------------
bin/elasticsearch-plugin install analysis-icu

bin/elasticsearch-plugin list

bin/elasticsearch-plugin remove analysis-icu

Cluster Backup
-------------------
Repository                        Configuration type
Shared file system "type"         : "fs"
Read-only URL "type"              : "url"
S3 "type"                         : "s3"
HDFS "type"                       : "hdfs"
Azure "type"                      : "azure"
Google Cloud Storage "type"       : "gcs"


Registering a Repository
--------------------------------------
path.repo: /mnt/my_repo_folder
needs to be added to elasticsearch.yml on all nodes

register a filesystem repository named “my_repo”

PUT _snapshot/my_repo
{
  "type": "fs",
   "settings": {
      "location": "/mnt/my_repo_folder"
    }
}

File System Repository Settings
----------------------------------
PUT _snapshot/my_repo
{
  "type": "fs",
    "settings": {
      "location": "/mnt/my_repo_folder",
      "compress": true,
      "max_restore_bytes_per_sec": "40mb",
      "max_snapshot_bytes_per_sec": "40mb"
    }
}

Taking a Snapshot (unique snapshot name)
----------------------------------
PUT _snapshot/my_repo/my_snapshot_1

Note:snapshot includes all open indices

Specifying Indices
-------------------
PUT _snapshot/my_repo/my_logs_snapshot_1
{
  "indices": "logs-*",
  "ignore_unavailable": true,
  "include_global_state": true
}

GET _snapshot/my_repo/my_snapshot_2/_status

PUT _snapshot/my_repo/my_logs_snapshot_2?wait_for_completion=true
{
  ...
}

Get information about snapshots in a repo:
---------------------------------------------
GET _snapshot/my_repo/_all
GET _snapshot/my_repo/my_snapshot_1
DELETE _snapshot/my_repo/my_snapshot_1

Restoring from a Snapshot
--------------------------
POST _snapshot/my_repo/my_snapshot_2/_restore

POST _snapshot/my_repo/my_snapshot_2/_restore
{
  "indices": "logs-*",
  "ignore_unavailable": true,
  "include_global_state": false
}

Renaming Indices
------------------
POST _snapshot/my_repo/my_snapshot_2/_restore
{
  "indices": "logs-*",
  "ignore_unavailable": true,
  "include_global_state": false,
  "rename_pattern": "logs-(.+)",
  "rename_replacement": "restored-logs-$1"
}

Monitoring Options
------------------
‒ Node Stats: _nodes/stats
‒ Cluster Stats: _cluster/stats
‒ Index Stats: my_index/_stats
‒ Pending Cluster Tasks API: _cluster/pending_tasks

GET _cluster/stats
GET _nodes/stats
GET _nodes/node1/stats
GET my_index/_stats
GET _stats  #You can get the stats from all indices in one request:

Task Management API
---------------------
GET _cluster/pending_tasks

GET _tasks  # You can also use this API to cancel a task

The cat API - Human readable format
-------------------------------------
GET _cat/nodes
GET _cat/nodes?v&h=name,disk.avail,search.query_total,heap.percent


Monitoring
-------------
‒ xpack.monitoring.collection.interval defaults to 10 seconds

Install Monitoring
--------------------
ES_HOME/bin/elasticsearch-plugin install x-pack
KIBANA_HOME/bin/kibana-plugin install x-pack

If Monitoring is on a different cluster than the one being monitored,
---------------------------------------------------------------------
xpack.monitoring.exporters:
  id1:
    type: http
    host: ["http://monitoring_cluster:9200"]
    auth.username: username
    auth.password: changeme

Configuring Monitoring
------------------------
‒ which can be configured in elasticsearch.yml:

xpack.monitoring.collection.indices : The indices to collect data from. Defaults to all indices, but can be a comma-separated list.
xpack.monitoring.collection.interval : How often data samples are collected. Defaults to 10s
xpack.monitoring.history.duration : How long before indices created by Monitoring are automatically deleted. Defaults to 7d.

Full list of monitoring is https://www.elastic.co/guide/en/x-pack/current/monitoring-settings.html


Handy Query Performance Commands
To get the wire time:
(time curl -s -XGET host:9200/tweets*/_search) 2>&1 > /dev/null | grep real | awk '{print $2}'
To get the took time:
curl -s -XGET 'host:9200/logstash-*/_search?pretty' | head | grep '"took"' | awk '{print $3}'

The Search Slow Log (in log4j.properties)
----------------------------------------
‒ The log file is already configured in log4j2.properties, but disabled by default

PUT my_index/_settings
{
  "index.search.slowlog.threshold" : {
    "query.warn" : "10s",
    "query.info" : "5s",
    "query.debug" : "2s",
    "query.trace" : "0s",
    "fetch.warn" : "1s",
    "fetch.info" : "800ms",
    "fetch.debug" : "500ms",
    "fetch.trace" : "0s"
  }
}

The Indexing Slow Log
-------------------------------------------------
‒ already configured in log4j2.properties

PUT my_index/_settings
{
  "index.indexing.slowlog" : {
    "threshold.index" : {
        "warn" : "10s",
        "info" : "5s",
        "debug" : "2s",
        "trace" : "0s"
    },
    "level" : "trace",
    "source" : 1000
    }
  }

Thread Pool Queues
-----------------------
GET _nodes/thread_pool
GET _nodes/stats/thread_pool
GET _cat/thread_pool?v

The hot_threads API
-----------------------------
GET _nodes/hot_threads
GET _nodes/<node_name>/hot_threads

Logstash Monitoring
-------------------------------
bin/logstash-plugin install x-pack

configure in your logstash.yml
------------------------------------
xpack.monitoring.elasticsearch.url: "http://PRODUCTION:9200"
xpack.monitoring.elasticsearch.username: "logstash_system"
xpack.monitoring.elasticsearch.password: "changeme"

Steps for a Rolling Upgrade
----------------------------------
Step 0: backup your cluster!
To perform a rolling upgrade:
1. stop non-essential indexing
2. disable shard allocation
3. stop and upgrade one node (including plugins)
4. start the node
5. re-enable shard allocation and wait
6. GOTO step 2 (until all nodes are updated)
-------------------------------------

Rolling Upgrade
• Step 1: stop indexing data (if possible)
‒ if you still need to index, that is OK but shard recovery will take longer

• Step 2: Disable shard allocation
PUT _cluster/settings
{
  "transient": {
     "cluster.routing.allocation.enable" : "none"
  }
}
POST _flush/synced

• Step 3: Install the new version:
‒ stop the node you want to upgrade (upgrade master nodes first!)
‒ install the new version (.zip, RPM, DEB, MSI)
‒ copy the config directory from the old location to the new one
‒ configure the new node to use the old data directory
‒ upgrade any plugins if applicable

• Step 4: Start the node up again
‒ and wait for it to join the cluster

• Step 5: Reenable shard allocation,
PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.enable" : "all"
  }
}
• then wait for the cluster to be green again:
‒ if green is not possible, check there are no initializing or relocating shards before continuing
GET _cat/health
Repeat for Each Node
• Step 6 is to start the process over for the next node
